{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e516ef",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2302b149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>æ³°å¦çš„Iç»ˆäºä¸Šâ€¦â€¦ä¸Šäº†ç„¶è€Œæˆ‘å¡å®åœ¨ç‹ çƒ‚</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>åƒå®Œæ™šé¥­å°±æ”¶æ‹¾æ´—ç¢—ï¼Œä¸Šæ¥¼é¡¶æ”¶è¡£æœå è¡£æœï¼Œç»™ä¹Œé¾ŸğŸ¢æ¢æ°´ç„¶åæ´—æ¾¡å‡ºæ¥åœ¨åºŠä¸Šç©æ‰‹æœºï¼Œç»“æœæŸäººå«æˆ‘å¿«...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å¤©çŒ«ç›’å­è‡ªè¯´è‡ªè¯æŠŠæˆ‘ä¸‹çš„ç›´æ’­APPå’ŒVSTè¿˜æœ‰æ²™å‘ç®¡å®¶è¿™äº›å…¨åˆ äº†å¼ºç›—å•Šéº»ç—¹ç›’å­è¿˜èƒ½å¼ºåˆ¶åˆ é™¤çš„...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>èƒ½ä¸èƒ½æ¥ç‚¹åƒåä¸€æœˆçš„æ¸©åº¦å•Šï¼Œçƒ­æ­»äºº</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3ç»†å“äºŒæ‹æˆå°±ä»â€œåˆåˆ»çš„åºè¨€é‡Œï¼Œå¯ä»¥çŸ¥é“å‡Œæ¿›åˆæ˜¯ç”±äºçœ‹åˆ°å†¯æ¢¦é¾™æ‰€ç¼–è¾‘çš„â€œä¸‰è¨€è¡Œä¸–é¢‡æ·ï¼Œå› è€Œåœ¨...</td>\n",
       "      <td>neural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                                æ³°å¦çš„Iç»ˆäºä¸Šâ€¦â€¦ä¸Šäº†ç„¶è€Œæˆ‘å¡å®åœ¨ç‹ çƒ‚   angry\n",
       "1  åƒå®Œæ™šé¥­å°±æ”¶æ‹¾æ´—ç¢—ï¼Œä¸Šæ¥¼é¡¶æ”¶è¡£æœå è¡£æœï¼Œç»™ä¹Œé¾ŸğŸ¢æ¢æ°´ç„¶åæ´—æ¾¡å‡ºæ¥åœ¨åºŠä¸Šç©æ‰‹æœºï¼Œç»“æœæŸäººå«æˆ‘å¿«...   angry\n",
       "2  å¤©çŒ«ç›’å­è‡ªè¯´è‡ªè¯æŠŠæˆ‘ä¸‹çš„ç›´æ’­APPå’ŒVSTè¿˜æœ‰æ²™å‘ç®¡å®¶è¿™äº›å…¨åˆ äº†å¼ºç›—å•Šéº»ç—¹ç›’å­è¿˜èƒ½å¼ºåˆ¶åˆ é™¤çš„...   angry\n",
       "3                                  èƒ½ä¸èƒ½æ¥ç‚¹åƒåä¸€æœˆçš„æ¸©åº¦å•Šï¼Œçƒ­æ­»äºº   angry\n",
       "4  3ç»†å“äºŒæ‹æˆå°±ä»â€œåˆåˆ»çš„åºè¨€é‡Œï¼Œå¯ä»¥çŸ¥é“å‡Œæ¿›åˆæ˜¯ç”±äºçœ‹åˆ°å†¯æ¢¦é¾™æ‰€ç¼–è¾‘çš„â€œä¸‰è¨€è¡Œä¸–é¢‡æ·ï¼Œå› è€Œåœ¨...  neural"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "data_train = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\train.xlsx')\n",
    "# data_train.head()\n",
    "# data_train['emotion'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3158d86",
   "metadata": {},
   "source": [
    "## 1.2 Get plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1994aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "angry       6652\n",
      "fear         978\n",
      "happy       4310\n",
      "neural      4602\n",
      "sad         4026\n",
      "surprise    1646\n",
      "Name: emotion, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "the sentence length of 0.91 quantile: 89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from itertools import accumulate\n",
    "\n",
    "#  è®¾ç½®matplotlibç»˜å›¾æ—¶çš„å­—ä½“\n",
    "my_font = font_manager.FontProperties(fname='D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\ARHei.ttf')\n",
    "\n",
    "#  ç»Ÿè®¡å¥å­é•¿åº¦åŠå‡ºç°æ¬¡æ•°çš„é¢‘æ•°\n",
    "# df = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\train.xlsx')\n",
    "print(data_train.groupby('emotion')['emotion'].count())\n",
    "\n",
    "print(type(data_train.text))\n",
    "data_train['length'] = data_train.text.astype(str).apply(len)\n",
    "# print(df)\n",
    "len_df = data_train.groupby('length').count()\n",
    "sent_length = len_df.index.tolist()\n",
    "sent_freq = len_df['text'].tolist()\n",
    "\n",
    "#  ç»˜åˆ¶å¥å­é•¿åº¦åŠå‡ºç°é¢‘æ•°ç»Ÿè®¡å›¾\n",
    "plt.bar(sent_length, sent_freq)\n",
    "plt.title(\"Plot of sentence length and frequency\", fontproperties=my_font)\n",
    "plt.xlabel(\"Sentence length\", fontproperties=my_font)\n",
    "plt.ylabel(\"Frequency of sentence length\", fontproperties=my_font)\n",
    "plt.savefig(\"./Plot of sentence length and frequency.png\")\n",
    "plt.close()\n",
    "\n",
    "#  ç»˜åˆ¶å¥å­é•¿åº¦ç´¯è®¡åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰\n",
    "sent_pentage_list = [(count / sum(sent_freq)) for count in accumulate(sent_freq)]\n",
    "\n",
    "# ç»˜åˆ¶CDF\n",
    "plt.plot(sent_length, sent_pentage_list)\n",
    "\n",
    "#  å¯»æ‰¾åˆ†ä½ç‚¹ä¸ºquantileçš„å¥å­é•¿åº¦\n",
    "quantile = 0.91\n",
    "# print(list(sent_pentage_list))\n",
    "for length, per in zip(sent_length, sent_pentage_list):\n",
    "    if round(per, 2) == quantile:\n",
    "        index = length\n",
    "        break\n",
    "print('\\nthe sentence length of %s quantile: %d' % (quantile, index))\n",
    "\n",
    "# ç»˜åˆ¶å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾\n",
    "plt.plot(sent_length, sent_pentage_list)\n",
    "plt.hlines(quantile, 0, index, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.vlines(index, 0, quantile, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.text(0, quantile, str(quantile))\n",
    "plt.text(index, 0, str(index))\n",
    "plt.title(\"Plot of sentence length cumulative distribution function\", fontproperties=my_font)\n",
    "plt.xlabel(\"Sentence length\", fontproperties=my_font)\n",
    "plt.ylabel(\"Cumulative distribution of sentence length\", fontproperties=my_font)\n",
    "plt.savefig(\"./Plot of sentence length cumulative distribution function.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b665c",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa051031",
   "metadata": {},
   "source": [
    "## 2.1 Label sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b58f1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def emotion2sentiment(emotion):\n",
    "    if emotion == 'angry':\n",
    "        return 0\n",
    "    elif emotion == 'fear':\n",
    "        return 1\n",
    "    elif emotion == 'sad':\n",
    "        return 2\n",
    "    elif emotion == 'neural':\n",
    "        return 3\n",
    "    elif emotion == 'surprise':\n",
    "        return 4\n",
    "    elif emotion == 'happy':\n",
    "        return 5\n",
    "    \n",
    "def sentiment2emotion(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"angry\"\n",
    "    elif sentiment == 1:\n",
    "        return \"fear\"\n",
    "    elif sentiment == 2:\n",
    "        return \"sad\"\n",
    "    elif sentiment == 3:\n",
    "        return \"neural\"\n",
    "    elif sentiment == 4:\n",
    "        return \"surprise\"\n",
    "    elif sentiment == 5:\n",
    "        return \"happy\"\n",
    "    \n",
    "data_train['sentiment'] = data_train.emotion.apply(emotion2sentiment)\n",
    "print(type(data_train.emotion))\n",
    "# data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198df1b",
   "metadata": {},
   "source": [
    "## 2.2 Remove symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d543ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_symbol(s_line):\n",
    "#     # å‰”é™¤è‹±æ–‡ã€æ•°å­—ï¼Œä»¥åŠç©ºæ ¼\n",
    "#     special_regex = re.compile(r\"[a-zA-Z0-9\\s]+\")\n",
    "#     # å‰”é™¤è‹±æ–‡æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šç¬¦å·\n",
    "#     en_regex = re.compile(r\"[.â€¦{|}#$%&\\'()*+,!-_./:~^;<=>?@â˜…â—ï¼Œã€‚]+\")\n",
    "#     # å‰”é™¤ä¸­æ–‡æ ‡ç‚¹ç¬¦å·\n",
    "#     zn_regex = re.compile(r\"[ã€Šã€‹ã€ï¼Œâ€œâ€ï¼›ï½ï¼Ÿï¼ï¼šï¼ˆï¼‰ã€ã€‘]+\")\n",
    "\n",
    "#     s_line = special_regex.sub(r\"\", s_line)\n",
    "#     s_line = en_regex.sub(r\"\", s_line)\n",
    "#     s_line = zn_regex.sub(r\"\", s_line)\n",
    "#     return s_line\n",
    "\n",
    "\n",
    "def remove_symbol(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    rule = re.compile(u\"[^a-zA-Z0-9\\u4E00-\\u9FA5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "\n",
    "\n",
    "data_train['remove_symbol_text'] = data_train.text.astype(str).apply(remove_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987aa5d0",
   "metadata": {},
   "source": [
    "## 2.3 Cut words by jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31afc8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Gao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.440 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "\n",
    "data_train['cut_text'] = data_train['remove_symbol_text'].astype(str).apply(chinese_word_cut)\n",
    "# data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dad89c",
   "metadata": {},
   "source": [
    "## 2.3 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3078b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data_train['cut_text']\n",
    "# y = data_train.sentiment\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=22)\n",
    "# X_train = data_train['cut_text']\n",
    "# y_train = data_train.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f92ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "stop_words_file = 'å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "print(type(stopwords))\n",
    "vect = CountVectorizer(max_df = 0.8, \n",
    "                       min_df = 3, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=stopwords)\n",
    "# vect = TfidfVectorizer(use_idf=True, smooth_idf=True,max_df = 0.8, \n",
    "#                        min_df = 3, \n",
    "#                        token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "#                        stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e08741b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ac</th>\n",
       "      <th>ampquot</th>\n",
       "      <th>and</th>\n",
       "      <th>anti</th>\n",
       "      <th>app</th>\n",
       "      <th>apple</th>\n",
       "      <th>baby</th>\n",
       "      <th>bb</th>\n",
       "      <th>bgm</th>\n",
       "      <th>bitch</th>\n",
       "      <th>...</th>\n",
       "      <th>é¼»å­”</th>\n",
       "      <th>é¼»å±</th>\n",
       "      <th>é¼»æ¢</th>\n",
       "      <th>é¼»æ¶•</th>\n",
       "      <th>é¼»ç‚</th>\n",
       "      <th>é½å…¨</th>\n",
       "      <th>é½é²æ™šæŠ¥</th>\n",
       "      <th>é½¿è½®</th>\n",
       "      <th>é¾Œé¾Š</th>\n",
       "      <th>é¾Ÿé€Ÿ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 12371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ac  ampquot  and  anti  app  apple  baby  bb  bgm  bitch  ...  é¼»å­”  é¼»å±  é¼»æ¢  \\\n",
       "0   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "1   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "2   0        0    0     0    1      0     0   0    0      0  ...   0   0   0   \n",
       "3   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "4   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "\n",
       "   é¼»æ¶•  é¼»ç‚  é½å…¨  é½é²æ™šæŠ¥  é½¿è½®  é¾Œé¾Š  é¾Ÿé€Ÿ  \n",
       "0   0   0   0     0   0   0   0  \n",
       "1   0   0   0     0   0   0   0  \n",
       "2   0   0   0     0   0   0   0  \n",
       "3   0   0   0     0   0   0   0  \n",
       "4   0   0   0     0   0   0   0  \n",
       "\n",
       "[5 rows x 12371 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = pd.DataFrame(vect.fit_transform(X_train).toarray(), columns=vect.get_feature_names_out())\n",
    "# test.head()\n",
    "# vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf1433",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bca61",
   "metadata": {},
   "source": [
    "## 3.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b681ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', 'â‘ â‘ ', 'â‘ â‘¡', 'â‘ â‘¢', 'â‘ â‘£', 'â‘ â‘¤', 'â‘ â‘¥', 'â‘ â‘¦', 'â‘ â‘§', 'â‘ â‘¨', 'â‘ ï½', 'â‘ ï½‚', 'â‘ ï½ƒ', 'â‘ ï½„', 'â‘ ï½…', 'â‘ ï½†', 'â‘ ï½‡', 'â‘ ï½ˆ', 'â‘ ï½‰', 'â‘ ï½', 'â‘¡â‘ ', 'â‘¡â‘¡', 'â‘¡â‘¢', 'â‘¡â‘£', 'â‘¡â‘¤', 'â‘¡â‘¥', 'â‘¡â‘¦', 'â‘¡â‘§', 'â‘¡â‘©', 'â‘¡ï½', 'â‘¡ï½‚', 'â‘¡ï½„', 'â‘¡ï½…', 'â‘¡ï½†', 'â‘¡ï½‡', 'â‘¡ï½ˆ', 'â‘¡ï½‰', 'â‘¡ï½Š', 'â‘¢â‘ ', 'â‘¢â‘©', 'â‘¢ï½', 'â‘¢ï½‚', 'â‘¢ï½ƒ', 'â‘¢ï½„', 'â‘¢ï½…', 'â‘¢ï½†', 'â‘¢ï½‡', 'â‘¢ï½ˆ', 'â‘£ï½', 'â‘£ï½‚', 'â‘£ï½ƒ', 'â‘£ï½„', 'â‘£ï½…', 'â‘¤ï½', 'â‘¤ï½‚', 'â‘¤ï½„', 'â‘¤ï½…', 'â‘¤ï½†', 'ï½Œï½‰', 'ï½šï½˜ï½†ï½‰ï½”ï½Œ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7809939677680742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "X_train = data_train['cut_text']\n",
    "y_train = data_train.sentiment\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aaa27a",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab4da026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(22214, 12371)\n",
      "(22214,)\n",
      "(22214,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_vect))\n",
    "print(X_train_vect.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5102b",
   "metadata": {},
   "source": [
    "# 4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94838e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505221462009363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cut_text</th>\n",
       "      <th>nb_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>çœŸæ­£å‡è‚¥å°±æ˜¯è¿™æ ·ï¼Œä½“é‡å˜åŒ–ä¸é‡è¦ï¼Œæ¯•ç«Ÿåˆ©å°¿å‰‚ä¹Ÿèƒ½åšåˆ°ï¼Œä½“å‹å˜åŒ–æ‰æ˜¯æœ€ç‰›çš„??ï¼ŒæŠ½æ²¹çš„èŠ‚å¥</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>çœŸæ­£ å‡è‚¥ å°±æ˜¯ è¿™æ · ï¼Œ ä½“é‡ å˜åŒ– ä¸ é‡è¦ ï¼Œ æ¯•ç«Ÿ åˆ©å°¿å‰‚ ä¹Ÿ èƒ½ åšåˆ° ï¼Œ ä½“å‹ ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>è¸é©¬çš„ï¼Œä¹‹å‰åºŠå«ç¡çš„ä¸èˆ’æœï¼Œä¹°äº†ä¸ªæ–°åºŠå«ï¼Œæ¢ä¸Šçš„æ—¶å€™å‘ç°ï¼ŒåŸæ¥æ˜¯ä¹‹å‰çš„åºŠå«å«åäº†â€¦â€¦æœ€è¿‘å¿«åŠ...</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>è¸é©¬ çš„ ï¼Œ ä¹‹å‰ åºŠå« ç¡ çš„ ä¸ èˆ’æœ ï¼Œ ä¹° äº† ä¸ª æ–° åºŠå« ï¼Œ æ¢ä¸Š çš„ æ—¶å€™ å‘...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>é«˜ä¸­æ—¶æœ€å¥½çš„æœ‹å‹ï¼Œåˆä½é™¢äº†åˆè¦åšå¼€é¢…æ‰‹æœ¯ï¼Œè²Œä¼¼æƒ…å†µæ›´ç³Ÿï¼Œå¥¹å¥½å®³æ€•ï¼Œå¥¹è¯´ï¼šâ€œè¿˜æœ‰å¾ˆå¤šæƒ³åšçš„äº‹æƒ…...</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>é«˜ä¸­ æ—¶ æœ€å¥½ çš„ æœ‹å‹ ï¼Œ åˆ ä½é™¢ äº† åˆ è¦ åš å¼€é¢… æ‰‹æœ¯ ï¼Œ è²Œä¼¼ æƒ…å†µ æ›´ç³Ÿ ï¼Œ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>å¾æµ·ä¹”æ›¾ç»åœ¨è®¿è°ˆé‡Œæè¿°è¿‡è‡ªå·±çš„è¿™æ®µç»å†ï¼Œè´µå·æ·±å±±çš„é›¨å¤œé‡Œæ‹æ‘„ä¸€éƒ¨è¿™æ ·é¢˜æçš„å½±ç‰‡ï¼Œæƒ³å¿…æ„Ÿå—ä¸€å®š...</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>å¾æµ· ä¹” æ›¾ç» åœ¨ è®¿è°ˆ é‡Œ æè¿° è¿‡ è‡ªå·± çš„ è¿™æ®µ ç»å† ï¼Œ è´µå· æ·±å±± çš„ é›¨ å¤œé‡Œ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>æˆ‘å¥½ç¾¡æ…•ä½ èƒ½æŒ¥éœç°å®æ”¹å˜å‘½è¿ï¼›ä½ å½·å¾¨åœ¨æˆ‘éœ€ä»°æœ›çš„å¤©ç©ºä¸­ï¼›ä½ èƒ½é£å¾—æ›´é«˜æ›´è¿œã€è¿™ä¸€åˆ‡æˆ‘å¹¶ä¸æ˜¯é¥ä¸...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>æˆ‘ å¥½ç¾¡æ…• ä½  èƒ½ æŒ¥éœ ç°å® æ”¹å˜å‘½è¿ ï¼› ä½  å½·å¾¨ åœ¨ æˆ‘ éœ€ ä»°æœ› çš„ å¤©ç©º ä¸­ ï¼› ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>å¥³äººç©ç›¸ä¿¡è‡ªå·±åŠ›é‡æ— ç©·å¤§é¥®æ°´æœºä¸Šæ°´æ²¡æœ‰ä»€ä¹ˆéš¾åº¦ã€‚çºªå¿µä¸€ä¸‹ã€‚</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>å¥³äºº ç© ç›¸ä¿¡ è‡ªå·± åŠ›é‡ æ— ç©·å¤§ é¥®æ°´æœº ä¸Š æ°´ æ²¡æœ‰ ä»€ä¹ˆ éš¾åº¦ ã€‚ çºªå¿µ ä¸€ä¸‹ ã€‚</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ä¸Šä¸€ç™¾æ–¤çš„æ„Ÿå—å°±æ˜¯ä¸ç©¿é‹ç«™åœ¨åœ°æ¿ä¸Šè„šä¸«éƒ½ç–¼ã€‚</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>ä¸Š ä¸€ç™¾æ–¤ çš„ æ„Ÿå— å°±æ˜¯ ä¸ ç©¿é‹ ç«™ åœ¨ åœ°æ¿ ä¸Š è„šä¸« éƒ½ ç–¼ ã€‚</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>åœ¨è¿™ä¸ªç»ˆçº§ç›®æ ‡ä¸‹ï¼Œä¸­åŒ»æ˜¯ç”¨ç²¾æ°”å­¦è¯´ã€é˜´é˜³å­¦è¯´å’Œäº”è¡Œå­¦è¯´ï¼Œè¿™ä¸‰å¤§æ¥è‡ªä¸­å›½å¤å…¸å“²å­¦çš„ç†è®ºï¼Œæ¥å…·ä½“...</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "      <td>åœ¨ è¿™ä¸ª ç»ˆçº§ ç›®æ ‡ ä¸‹ ï¼Œ ä¸­åŒ» æ˜¯ ç”¨ ç²¾æ°” å­¦è¯´ ã€ é˜´é˜³ å­¦è¯´ å’Œ äº”è¡Œ å­¦è¯´ ï¼Œ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>å·å—å¥‰è¿™ç§åè¿œè·¯çº¿å‡ºç°ä½¿é¦†è½¦ï¼Œä¼šä¸ä¼šæ˜¯é—´è°ï¼Ÿ</td>\n",
       "      <td>surprise</td>\n",
       "      <td>4</td>\n",
       "      <td>å·å— å¥‰ è¿™ç§ åè¿œ è·¯çº¿ å‡ºç° ä½¿é¦† è½¦ ï¼Œ ä¼š ä¸ä¼š æ˜¯ é—´è° ï¼Ÿ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>èŠ±å²—é—ªé•¿å²©çš„ææ–™ç‰¹å¾ä¸èŠ±å²—å²©ç±»ä¼¼ã€‚</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "      <td>èŠ±å²— é—ªé•¿å²© çš„ ææ–™ ç‰¹å¾ ä¸ èŠ±å²—å²© ç±»ä¼¼ ã€‚</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>è¿™æ‰‹æœºæœ‰ä¸€ç‚¹ä¸å¥½ï¼Œä¸è§£å¼€å¯†ç é”å±…ç„¶èƒ½æ‹ç…§</td>\n",
       "      <td>surprise</td>\n",
       "      <td>4</td>\n",
       "      <td>è¿™ æ‰‹æœº æœ‰ ä¸€ç‚¹ ä¸å¥½ ï¼Œ ä¸ è§£å¼€ å¯†ç é” å±…ç„¶ èƒ½ æ‹ç…§</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ä»Šå¤©æ˜¯å›å›½åæœ€æ”¾æ¾çš„ä¸€å¤©äº†ï¼Œå…ˆæ˜¯æ·˜åˆ°å¾ˆå–œæ¬¢çš„ç²‰çº¢å°é‹ï¼Œåæ˜¯æ•´æ•´ç¡äº†ä¸€ä¸‹åˆçš„è§‰ã€‚ç„¶åæœ‹å‹å¸¦å»åƒ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>ä»Šå¤© æ˜¯ å›å›½ å æœ€ æ”¾æ¾ çš„ ä¸€å¤© äº† ï¼Œ å…ˆæ˜¯ æ·˜åˆ° å¾ˆ å–œæ¬¢ çš„ ç²‰çº¢ å°é‹ ï¼Œ å...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>å¬åˆ°ç†Ÿæ‚‰çš„æ—‹å¾‹ï¼Œæƒ³èµ·é«˜ä¸­å…¥å­¦ä¹‹å‰çš„é‚£ä¸ªå†›è®­ï¼Œå¥½åƒé‚£ä¸€å¹´æ•´ä¸ªå¤å¤©éƒ½æ˜¯ç«çƒ­çš„ï¼Œæˆ‘è¢«æ™’å¾—é»é»‘é»é»‘ï¼Œ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>å¬åˆ° ç†Ÿæ‚‰ çš„ æ—‹å¾‹ ï¼Œ æƒ³èµ· é«˜ä¸­ å…¥å­¦ ä¹‹å‰ çš„ é‚£ä¸ª å†›è®­ ï¼Œ å¥½åƒ é‚£ ä¸€å¹´ æ•´ä¸ª ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>å¿«é€’è¢«æŸä¸ªäººè‡ªè¯´è‡ªè¯æ‹†å¼€äº†ï¼Œå¿ƒé‡Œåƒåƒäº†è‹è‡å±ä¸€æ ·çªç‘Ÿ</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>å¿«é€’ è¢« æŸä¸ª äºº è‡ªè¯´è‡ªè¯ æ‹†å¼€ äº† ï¼Œ å¿ƒé‡Œ åƒ åƒ äº† è‹è‡ å± ä¸€æ · çªç‘Ÿ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>åšåº“å­˜è¡¨ï¼Œçªç„¶æƒŠè§‰ï¼Œç«Ÿç„¶å°±å¿«è¦è¿›å…¥12æœˆäº†ï¼Œæ€ç´¢äº†ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆåå°„å¼§è¿™ä¹ˆé•¿ï¼Ÿç¦ï¼å»ºï¼è¿˜ï¼ç©¿ï¼...</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>åš åº“å­˜ è¡¨ ï¼Œ çªç„¶ æƒŠè§‰ ï¼Œ ç«Ÿç„¶ å°± å¿«è¦ è¿›å…¥ 12 æœˆ äº† ï¼Œ æ€ç´¢ äº† ä¸€ä¸‹ ï¼Œ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>æ—¶é—´å¯ä»¥å¦‚åˆï¼Œæœ‰äº›äº‹æƒ…ï¼Œå´éš¾ä»¥å¦‚æ•…</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>æ—¶é—´ å¯ä»¥ å¦‚åˆ ï¼Œ æœ‰äº› äº‹æƒ… ï¼Œ å´ éš¾ä»¥ å¦‚æ•…</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>å±æœºæœ‰ä¸¤ç§ï¼Œå½“é›·æ›¼å…„å¼Ÿæˆ–ä¸¤æˆ¿å´©æºƒæ—¶ï¼Œäººä»¬ä¸ä¸€å®šé€ƒå‘é‡‘é“¶ï¼Œä»–ä»¬æ›´å¯èƒ½æŠ•å¥”ç¾å…ƒæˆ–ç¾å›½å›½å€ºï¼Œè€å¤«ç§°...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>å±æœº æœ‰ ä¸¤ç§ ï¼Œ å½“ é›·æ›¼ å…„å¼Ÿ æˆ– ä¸¤æˆ¿ å´©æºƒ æ—¶ ï¼Œ äººä»¬ ä¸ ä¸€å®š é€ƒå‘ é‡‘é“¶ ï¼Œ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ç¬¬ä¸€æ¬¡ä¼¼ä¹ä¹Ÿæ²¡é‚£ä¹ˆå“äººï¼Œæ€»æ˜¯èº²é¿ç€æ–°äº‹ç‰©ï¼Œå…¶å®ç»å†è¿‡æ‰å‘ç°åŸæ¥è¿˜æŒºæœ‰è¶£çš„ï¼Œæ€§æ ¼å¥½åŠ¨çš„äººæ€ä¹ˆèƒ½...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>ç¬¬ä¸€æ¬¡ ä¼¼ä¹ ä¹Ÿ æ²¡ é‚£ä¹ˆ å“äºº ï¼Œ æ€»æ˜¯ èº²é¿ ç€ æ–° äº‹ç‰© ï¼Œ å…¶å® ç»å† è¿‡æ‰ å‘ç° ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ä¸èƒ½çœ‹å¤ªè¿‡æ‚²ä¼¤çš„ç”µå½±ï¼Œæƒ…ç»ªéƒ½å˜å¾—æœ‰ç‚¹å¿§ä¼¤ã€‚é‡Œæ˜‚é‚£ä¹ˆå¥½ï¼Œä»–ä¸è¯¥æ­»ï¼Œä»–å€¼å¾—æ›´å¥½çš„ç”Ÿæ´»</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>ä¸èƒ½ çœ‹å¤ªè¿‡ æ‚²ä¼¤ çš„ ç”µå½± ï¼Œ æƒ…ç»ª éƒ½ å˜å¾— æœ‰ç‚¹ å¿§ä¼¤ ã€‚ é‡Œæ˜‚ é‚£ä¹ˆ å¥½ ï¼Œ ä»– ä¸...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ææ€–çš„å­˜åœ¨ï¼Œä¸ºä»€ä¹ˆæ²¡æœ‰äººç®¡ï¼Œå¤ªå“äººäº†ï¼</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>ææ€– çš„ å­˜åœ¨ ï¼Œ ä¸ºä»€ä¹ˆ æ²¡æœ‰ äººç®¡ ï¼Œ å¤ª å“äºº äº† ï¼</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   emotion  sentiment  \\\n",
       "0        çœŸæ­£å‡è‚¥å°±æ˜¯è¿™æ ·ï¼Œä½“é‡å˜åŒ–ä¸é‡è¦ï¼Œæ¯•ç«Ÿåˆ©å°¿å‰‚ä¹Ÿèƒ½åšåˆ°ï¼Œä½“å‹å˜åŒ–æ‰æ˜¯æœ€ç‰›çš„??ï¼ŒæŠ½æ²¹çš„èŠ‚å¥     happy          5   \n",
       "1   è¸é©¬çš„ï¼Œä¹‹å‰åºŠå«ç¡çš„ä¸èˆ’æœï¼Œä¹°äº†ä¸ªæ–°åºŠå«ï¼Œæ¢ä¸Šçš„æ—¶å€™å‘ç°ï¼ŒåŸæ¥æ˜¯ä¹‹å‰çš„åºŠå«å«åäº†â€¦â€¦æœ€è¿‘å¿«åŠ...     angry          0   \n",
       "2   é«˜ä¸­æ—¶æœ€å¥½çš„æœ‹å‹ï¼Œåˆä½é™¢äº†åˆè¦åšå¼€é¢…æ‰‹æœ¯ï¼Œè²Œä¼¼æƒ…å†µæ›´ç³Ÿï¼Œå¥¹å¥½å®³æ€•ï¼Œå¥¹è¯´ï¼šâ€œè¿˜æœ‰å¾ˆå¤šæƒ³åšçš„äº‹æƒ…...       sad          2   \n",
       "3   å¾æµ·ä¹”æ›¾ç»åœ¨è®¿è°ˆé‡Œæè¿°è¿‡è‡ªå·±çš„è¿™æ®µç»å†ï¼Œè´µå·æ·±å±±çš„é›¨å¤œé‡Œæ‹æ‘„ä¸€éƒ¨è¿™æ ·é¢˜æçš„å½±ç‰‡ï¼Œæƒ³å¿…æ„Ÿå—ä¸€å®š...       sad          2   \n",
       "4   æˆ‘å¥½ç¾¡æ…•ä½ èƒ½æŒ¥éœç°å®æ”¹å˜å‘½è¿ï¼›ä½ å½·å¾¨åœ¨æˆ‘éœ€ä»°æœ›çš„å¤©ç©ºä¸­ï¼›ä½ èƒ½é£å¾—æ›´é«˜æ›´è¿œã€è¿™ä¸€åˆ‡æˆ‘å¹¶ä¸æ˜¯é¥ä¸...     happy          5   \n",
       "5                       å¥³äººç©ç›¸ä¿¡è‡ªå·±åŠ›é‡æ— ç©·å¤§é¥®æ°´æœºä¸Šæ°´æ²¡æœ‰ä»€ä¹ˆéš¾åº¦ã€‚çºªå¿µä¸€ä¸‹ã€‚     happy          5   \n",
       "6                              ä¸Šä¸€ç™¾æ–¤çš„æ„Ÿå—å°±æ˜¯ä¸ç©¿é‹ç«™åœ¨åœ°æ¿ä¸Šè„šä¸«éƒ½ç–¼ã€‚       sad          2   \n",
       "7   åœ¨è¿™ä¸ªç»ˆçº§ç›®æ ‡ä¸‹ï¼Œä¸­åŒ»æ˜¯ç”¨ç²¾æ°”å­¦è¯´ã€é˜´é˜³å­¦è¯´å’Œäº”è¡Œå­¦è¯´ï¼Œè¿™ä¸‰å¤§æ¥è‡ªä¸­å›½å¤å…¸å“²å­¦çš„ç†è®ºï¼Œæ¥å…·ä½“...    neural          3   \n",
       "8                              å·å—å¥‰è¿™ç§åè¿œè·¯çº¿å‡ºç°ä½¿é¦†è½¦ï¼Œä¼šä¸ä¼šæ˜¯é—´è°ï¼Ÿ  surprise          4   \n",
       "9                                   èŠ±å²—é—ªé•¿å²©çš„ææ–™ç‰¹å¾ä¸èŠ±å²—å²©ç±»ä¼¼ã€‚    neural          3   \n",
       "10                               è¿™æ‰‹æœºæœ‰ä¸€ç‚¹ä¸å¥½ï¼Œä¸è§£å¼€å¯†ç é”å±…ç„¶èƒ½æ‹ç…§  surprise          4   \n",
       "11  ä»Šå¤©æ˜¯å›å›½åæœ€æ”¾æ¾çš„ä¸€å¤©äº†ï¼Œå…ˆæ˜¯æ·˜åˆ°å¾ˆå–œæ¬¢çš„ç²‰çº¢å°é‹ï¼Œåæ˜¯æ•´æ•´ç¡äº†ä¸€ä¸‹åˆçš„è§‰ã€‚ç„¶åæœ‹å‹å¸¦å»åƒ...     happy          5   \n",
       "12  å¬åˆ°ç†Ÿæ‚‰çš„æ—‹å¾‹ï¼Œæƒ³èµ·é«˜ä¸­å…¥å­¦ä¹‹å‰çš„é‚£ä¸ªå†›è®­ï¼Œå¥½åƒé‚£ä¸€å¹´æ•´ä¸ªå¤å¤©éƒ½æ˜¯ç«çƒ­çš„ï¼Œæˆ‘è¢«æ™’å¾—é»é»‘é»é»‘ï¼Œ...     happy          5   \n",
       "13                         å¿«é€’è¢«æŸä¸ªäººè‡ªè¯´è‡ªè¯æ‹†å¼€äº†ï¼Œå¿ƒé‡Œåƒåƒäº†è‹è‡å±ä¸€æ ·çªç‘Ÿ     angry          0   \n",
       "14  åšåº“å­˜è¡¨ï¼Œçªç„¶æƒŠè§‰ï¼Œç«Ÿç„¶å°±å¿«è¦è¿›å…¥12æœˆäº†ï¼Œæ€ç´¢äº†ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆåå°„å¼§è¿™ä¹ˆé•¿ï¼Ÿç¦ï¼å»ºï¼è¿˜ï¼ç©¿ï¼...     angry          0   \n",
       "15                                  æ—¶é—´å¯ä»¥å¦‚åˆï¼Œæœ‰äº›äº‹æƒ…ï¼Œå´éš¾ä»¥å¦‚æ•…       sad          2   \n",
       "16  å±æœºæœ‰ä¸¤ç§ï¼Œå½“é›·æ›¼å…„å¼Ÿæˆ–ä¸¤æˆ¿å´©æºƒæ—¶ï¼Œäººä»¬ä¸ä¸€å®šé€ƒå‘é‡‘é“¶ï¼Œä»–ä»¬æ›´å¯èƒ½æŠ•å¥”ç¾å…ƒæˆ–ç¾å›½å›½å€ºï¼Œè€å¤«ç§°...      fear          1   \n",
       "17  ç¬¬ä¸€æ¬¡ä¼¼ä¹ä¹Ÿæ²¡é‚£ä¹ˆå“äººï¼Œæ€»æ˜¯èº²é¿ç€æ–°äº‹ç‰©ï¼Œå…¶å®ç»å†è¿‡æ‰å‘ç°åŸæ¥è¿˜æŒºæœ‰è¶£çš„ï¼Œæ€§æ ¼å¥½åŠ¨çš„äººæ€ä¹ˆèƒ½...     happy          5   \n",
       "18           ä¸èƒ½çœ‹å¤ªè¿‡æ‚²ä¼¤çš„ç”µå½±ï¼Œæƒ…ç»ªéƒ½å˜å¾—æœ‰ç‚¹å¿§ä¼¤ã€‚é‡Œæ˜‚é‚£ä¹ˆå¥½ï¼Œä»–ä¸è¯¥æ­»ï¼Œä»–å€¼å¾—æ›´å¥½çš„ç”Ÿæ´»       sad          2   \n",
       "19                                ææ€–çš„å­˜åœ¨ï¼Œä¸ºä»€ä¹ˆæ²¡æœ‰äººç®¡ï¼Œå¤ªå“äººäº†ï¼      fear          1   \n",
       "\n",
       "                                             cut_text  nb_result  \n",
       "0   çœŸæ­£ å‡è‚¥ å°±æ˜¯ è¿™æ · ï¼Œ ä½“é‡ å˜åŒ– ä¸ é‡è¦ ï¼Œ æ¯•ç«Ÿ åˆ©å°¿å‰‚ ä¹Ÿ èƒ½ åšåˆ° ï¼Œ ä½“å‹ ...          5  \n",
       "1   è¸é©¬ çš„ ï¼Œ ä¹‹å‰ åºŠå« ç¡ çš„ ä¸ èˆ’æœ ï¼Œ ä¹° äº† ä¸ª æ–° åºŠå« ï¼Œ æ¢ä¸Š çš„ æ—¶å€™ å‘...          4  \n",
       "2   é«˜ä¸­ æ—¶ æœ€å¥½ çš„ æœ‹å‹ ï¼Œ åˆ ä½é™¢ äº† åˆ è¦ åš å¼€é¢… æ‰‹æœ¯ ï¼Œ è²Œä¼¼ æƒ…å†µ æ›´ç³Ÿ ï¼Œ...          2  \n",
       "3   å¾æµ· ä¹” æ›¾ç» åœ¨ è®¿è°ˆ é‡Œ æè¿° è¿‡ è‡ªå·± çš„ è¿™æ®µ ç»å† ï¼Œ è´µå· æ·±å±± çš„ é›¨ å¤œé‡Œ ...          1  \n",
       "4   æˆ‘ å¥½ç¾¡æ…• ä½  èƒ½ æŒ¥éœ ç°å® æ”¹å˜å‘½è¿ ï¼› ä½  å½·å¾¨ åœ¨ æˆ‘ éœ€ ä»°æœ› çš„ å¤©ç©º ä¸­ ï¼› ...          5  \n",
       "5        å¥³äºº ç© ç›¸ä¿¡ è‡ªå·± åŠ›é‡ æ— ç©·å¤§ é¥®æ°´æœº ä¸Š æ°´ æ²¡æœ‰ ä»€ä¹ˆ éš¾åº¦ ã€‚ çºªå¿µ ä¸€ä¸‹ ã€‚          5  \n",
       "6                ä¸Š ä¸€ç™¾æ–¤ çš„ æ„Ÿå— å°±æ˜¯ ä¸ ç©¿é‹ ç«™ åœ¨ åœ°æ¿ ä¸Š è„šä¸« éƒ½ ç–¼ ã€‚          0  \n",
       "7   åœ¨ è¿™ä¸ª ç»ˆçº§ ç›®æ ‡ ä¸‹ ï¼Œ ä¸­åŒ» æ˜¯ ç”¨ ç²¾æ°” å­¦è¯´ ã€ é˜´é˜³ å­¦è¯´ å’Œ äº”è¡Œ å­¦è¯´ ï¼Œ ...          3  \n",
       "8                 å·å— å¥‰ è¿™ç§ åè¿œ è·¯çº¿ å‡ºç° ä½¿é¦† è½¦ ï¼Œ ä¼š ä¸ä¼š æ˜¯ é—´è° ï¼Ÿ          3  \n",
       "9                           èŠ±å²— é—ªé•¿å²© çš„ ææ–™ ç‰¹å¾ ä¸ èŠ±å²—å²© ç±»ä¼¼ ã€‚          3  \n",
       "10                    è¿™ æ‰‹æœº æœ‰ ä¸€ç‚¹ ä¸å¥½ ï¼Œ ä¸ è§£å¼€ å¯†ç é” å±…ç„¶ èƒ½ æ‹ç…§          0  \n",
       "11  ä»Šå¤© æ˜¯ å›å›½ å æœ€ æ”¾æ¾ çš„ ä¸€å¤© äº† ï¼Œ å…ˆæ˜¯ æ·˜åˆ° å¾ˆ å–œæ¬¢ çš„ ç²‰çº¢ å°é‹ ï¼Œ å...          5  \n",
       "12  å¬åˆ° ç†Ÿæ‚‰ çš„ æ—‹å¾‹ ï¼Œ æƒ³èµ· é«˜ä¸­ å…¥å­¦ ä¹‹å‰ çš„ é‚£ä¸ª å†›è®­ ï¼Œ å¥½åƒ é‚£ ä¸€å¹´ æ•´ä¸ª ...          2  \n",
       "13          å¿«é€’ è¢« æŸä¸ª äºº è‡ªè¯´è‡ªè¯ æ‹†å¼€ äº† ï¼Œ å¿ƒé‡Œ åƒ åƒ äº† è‹è‡ å± ä¸€æ · çªç‘Ÿ          0  \n",
       "14  åš åº“å­˜ è¡¨ ï¼Œ çªç„¶ æƒŠè§‰ ï¼Œ ç«Ÿç„¶ å°± å¿«è¦ è¿›å…¥ 12 æœˆ äº† ï¼Œ æ€ç´¢ äº† ä¸€ä¸‹ ï¼Œ...          0  \n",
       "15                         æ—¶é—´ å¯ä»¥ å¦‚åˆ ï¼Œ æœ‰äº› äº‹æƒ… ï¼Œ å´ éš¾ä»¥ å¦‚æ•…          2  \n",
       "16  å±æœº æœ‰ ä¸¤ç§ ï¼Œ å½“ é›·æ›¼ å…„å¼Ÿ æˆ– ä¸¤æˆ¿ å´©æºƒ æ—¶ ï¼Œ äººä»¬ ä¸ ä¸€å®š é€ƒå‘ é‡‘é“¶ ï¼Œ ...          3  \n",
       "17  ç¬¬ä¸€æ¬¡ ä¼¼ä¹ ä¹Ÿ æ²¡ é‚£ä¹ˆ å“äºº ï¼Œ æ€»æ˜¯ èº²é¿ ç€ æ–° äº‹ç‰© ï¼Œ å…¶å® ç»å† è¿‡æ‰ å‘ç° ...          1  \n",
       "18  ä¸èƒ½ çœ‹å¤ªè¿‡ æ‚²ä¼¤ çš„ ç”µå½± ï¼Œ æƒ…ç»ª éƒ½ å˜å¾— æœ‰ç‚¹ å¿§ä¼¤ ã€‚ é‡Œæ˜‚ é‚£ä¹ˆ å¥½ ï¼Œ ä»– ä¸...          2  \n",
       "19                     ææ€– çš„ å­˜åœ¨ ï¼Œ ä¸ºä»€ä¹ˆ æ²¡æœ‰ äººç®¡ ï¼Œ å¤ª å“äºº äº† ï¼          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\test.xlsx')\n",
    "data_test['sentiment'] = data_test.emotion.apply(emotion2sentiment)\n",
    "data_test['cut_text'] = data_test.text.astype(str).apply(chinese_word_cut)\n",
    "X_test = data_test['cut_text']\n",
    "y_test = data_test.sentiment\n",
    "X_test_vect = vect.transform(X_test)\n",
    "print(nb.score(X_test_vect, y_test))\n",
    "nb_result = nb.predict(X_test_vect)\n",
    "data_test['nb_result'] = nb_result\n",
    "data_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96712096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "m_tp: 1334.0\n",
      "m_fp: 635.0\n",
      "m_fn: 358.0\n",
      "m_tn: 3227.0\n",
      "fear\n",
      "m_tp: 123.0\n",
      "m_fp: 68.0\n",
      "m_fn: 119.0\n",
      "m_tn: 5244.0\n",
      "sad\n",
      "m_tp: 544.0\n",
      "m_fp: 519.0\n",
      "m_fn: 420.0\n",
      "m_tn: 4071.0\n",
      "neural\n",
      "m_tp: 785.0\n",
      "m_fp: 155.0\n",
      "m_fn: 362.0\n",
      "m_tn: 4252.0\n",
      "surprise\n",
      "m_tp: 141.0\n",
      "m_fp: 118.0\n",
      "m_fn: 299.0\n",
      "m_tn: 4996.0\n",
      "happy\n",
      "m_tp: 686.0\n",
      "m_fp: 446.0\n",
      "m_fn: 383.0\n",
      "m_tn: 4039.0\n",
      "overall accuracy: 0.6505221462009363\n",
      "overall precision: 0.6364590823240035\n",
      "overall recall: 0.5845942904835666\n",
      "overall f_measure: 0.6094251962099261\n",
      "angry\n",
      "0.8212099387828592\n",
      "0.6775012696800407\n",
      "0.7884160756501182\n",
      "0.7287626331603386\n",
      "fear\n",
      "0.9663305725603168\n",
      "0.643979057591623\n",
      "0.5082644628099173\n",
      "0.5681293302540416\n",
      "sad\n",
      "0.8309326611451207\n",
      "0.5117591721542804\n",
      "0.5643153526970954\n",
      "0.5367538233843118\n",
      "neural\n",
      "0.9069139359020526\n",
      "0.8351063829787234\n",
      "0.6843940714908456\n",
      "0.7522759942501198\n",
      "surprise\n",
      "0.9249189773136478\n",
      "0.5444015444015444\n",
      "0.32045454545454544\n",
      "0.4034334763948498\n",
      "happy\n",
      "0.8507382066978754\n",
      "0.6060070671378092\n",
      "0.6417212347988774\n",
      "0.62335302135393\n",
      "accuracy 0.6505221462009363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.68      0.79      0.73      1692\n",
      "        fear       0.64      0.51      0.57       242\n",
      "         sad       0.51      0.56      0.54       964\n",
      "      neural       0.84      0.68      0.75      1147\n",
      "    surprise       0.54      0.32      0.40       440\n",
      "       happy       0.61      0.64      0.62      1069\n",
      "\n",
      "    accuracy                           0.65      5554\n",
      "   macro avg       0.64      0.58      0.60      5554\n",
      "weighted avg       0.66      0.65      0.65      5554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_matrix(ls):\n",
    "    eval_matrix = np.zeros((6, 6))\n",
    "    for m_i in range(len(ls)):\n",
    "        m_row = ls['sentiment'][m_i]\n",
    "        m_col = ls['nb_result'][m_i]\n",
    "        eval_matrix[m_row][m_col] += 1\n",
    "    return eval_matrix\n",
    "#     print(eval_matrix_nb) \n",
    "\n",
    "def get_evaluation(matrix):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    tn = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f_measure = []\n",
    "    for m_i in range(6):\n",
    "        m_tp = matrix[m_i][m_i]\n",
    "        m_fp = np.sum(matrix, axis=0)[m_i] - matrix[m_i][m_i]\n",
    "        m_fn = np.sum(matrix, axis=1)[m_i] - matrix[m_i][m_i]\n",
    "        m_tn = np.sum(matrix) - m_tp - m_fp - m_fn\n",
    "        m_accuracy = (m_tp + m_tn) / (m_tp + m_fp + m_fn + m_tn)\n",
    "        m_precision = (m_tp) / (m_tp + m_fp)\n",
    "        m_recall = m_tp / (m_tp + m_fn)\n",
    "        m_f_measure = 2 * m_precision * m_recall / (m_precision + m_recall)\n",
    "        accuracy.append(m_accuracy)\n",
    "        precision.append(m_precision)\n",
    "        recall.append(m_recall)\n",
    "        f_measure.append(m_f_measure)\n",
    "        print(sentiment2emotion(m_i))\n",
    "        print(\"m_tp: \" + str(m_tp))\n",
    "        print(\"m_fp: \" + str(m_fp))\n",
    "        print(\"m_fn: \" + str(m_fn))\n",
    "        print(\"m_tn: \" + str(m_tn))\n",
    "    overall_accuracy = np.trace(matrix) / np.sum(matrix)\n",
    "    overall_precision = np.sum(precision) / len(precision)\n",
    "    overall_recall = np.sum(recall) / len(recall)\n",
    "    overall_f_measure = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "    return overall_accuracy, overall_precision, overall_recall, overall_f_measure, accuracy, precision, recall, f_measure\n",
    "\n",
    "eval_matrix_nb = get_evaluation_matrix(data_test)\n",
    "overall_accuracy_nb, overall_precision_nb, overall_recall_nb, overall_f_measure_nb, accuracy_nb, precision_nb, recall_nb, f_measure_nb = get_evaluation(eval_matrix_nb)\n",
    "print(\"overall accuracy: \" + str(overall_accuracy_nb))\n",
    "print(\"overall precision: \" + str(overall_precision_nb))\n",
    "print(\"overall recall: \" + str(overall_recall_nb))\n",
    "print(\"overall f_measure: \" + str(overall_f_measure_nb))\n",
    "for i in range(6):\n",
    "    emotion_type = sentiment2emotion(i)\n",
    "    print(emotion_type)\n",
    "    print(str(accuracy_nb[i]))\n",
    "    print(str(precision_nb[i]))\n",
    "    print(str(recall_nb[i]))\n",
    "    print(str(f_measure_nb[i]))\n",
    "#     print(\"accuracy: \" + str(accuracy_nb[i]))\n",
    "#     print(\"precision: \" + str(precision_nb[i]))\n",
    "#     print(\"recall: \" + str(recall_nb[i]))\n",
    "#     print(\"f_measure: \" + str(f_measure_nb[i]))\n",
    "\n",
    "\n",
    "from  sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "print('accuracy %s' % accuracy_score(data_test['nb_result'], data_test['sentiment']))\n",
    "# print(type(range(6)))\n",
    "tg_names = pd.Series(range(len(data_test.emotion.unique()))).apply(sentiment2emotion)\n",
    "print(classification_report(data_test['sentiment'], data_test['nb_result'], target_names=tg_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63fc25",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470a6cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the sentence:è¿™åœºç”µå½±å¥½å¥½çœ‹\n",
      "The emotion of the sentence is happy\n"
     ]
    }
   ],
   "source": [
    "test_input = input(\"input the sentence:\")\n",
    "# test_input = \"ç”µå½±çœŸå¥½çœ‹\"\n",
    "test_cut = chinese_word_cut(remove_symbol(test_input))\n",
    "# print(type(test_cut))\n",
    "test_vec = vect.transform([test_cut])\n",
    "# print(type(test_vec))\n",
    "res = nb.predict(test_vec)\n",
    "# print(res)\n",
    "print(\"The emotion of the sentence is \" + sentiment2emotion(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c65ad",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6981cd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6652, 3: 4602, 5: 4310, 2: 4026, 4: 1646, 1: 978})\n",
      "Counter({0: 1692, 3: 1147, 5: 1069, 2: 964, 4: 440, 1: 242})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "arr = Counter(y_train)\n",
    "print(arr)\n",
    "arr = Counter(y_test)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de0b166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5554.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(eval_matrix_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05688cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy', 'angry', 'sad', ..., 'neural', 'happy', 'happy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f393bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
