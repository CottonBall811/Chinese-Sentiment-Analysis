{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e516ef",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2302b149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>泰妍的I终于上……上了然而我卡实在狠烂</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>吃完晚饭就收拾洗碗，上楼顶收衣服叠衣服，给乌龟🐢换水然后洗澡出来在床上玩手机，结果某人叫我快...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>天猫盒子自说自话把我下的直播APP和VST还有沙发管家这些全删了强盗啊麻痹盒子还能强制删除的...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>能不能来点像十一月的温度啊，热死人</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3细品二拍成就从“初刻的序言里，可以知道凌濛初是由于看到冯梦龙所编辑的“三言行世颇捷，因而在...</td>\n",
       "      <td>neural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                                泰妍的I终于上……上了然而我卡实在狠烂   angry\n",
       "1  吃完晚饭就收拾洗碗，上楼顶收衣服叠衣服，给乌龟🐢换水然后洗澡出来在床上玩手机，结果某人叫我快...   angry\n",
       "2  天猫盒子自说自话把我下的直播APP和VST还有沙发管家这些全删了强盗啊麻痹盒子还能强制删除的...   angry\n",
       "3                                  能不能来点像十一月的温度啊，热死人   angry\n",
       "4  3细品二拍成就从“初刻的序言里，可以知道凌濛初是由于看到冯梦龙所编辑的“三言行世颇捷，因而在...  neural"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "data_train = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\train.xlsx')\n",
    "# data_train.head()\n",
    "# data_train['emotion'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3158d86",
   "metadata": {},
   "source": [
    "## 1.2 Get plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1994aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "angry       6652\n",
      "fear         978\n",
      "happy       4310\n",
      "neural      4602\n",
      "sad         4026\n",
      "surprise    1646\n",
      "Name: emotion, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "the sentence length of 0.91 quantile: 89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from itertools import accumulate\n",
    "\n",
    "#  设置matplotlib绘图时的字体\n",
    "my_font = font_manager.FontProperties(fname='D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\ARHei.ttf')\n",
    "\n",
    "#  统计句子长度及出现次数的频数\n",
    "# df = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\train.xlsx')\n",
    "print(data_train.groupby('emotion')['emotion'].count())\n",
    "\n",
    "print(type(data_train.text))\n",
    "data_train['length'] = data_train.text.astype(str).apply(len)\n",
    "# print(df)\n",
    "len_df = data_train.groupby('length').count()\n",
    "sent_length = len_df.index.tolist()\n",
    "sent_freq = len_df['text'].tolist()\n",
    "\n",
    "#  绘制句子长度及出现频数统计图\n",
    "plt.bar(sent_length, sent_freq)\n",
    "plt.title(\"Plot of sentence length and frequency\", fontproperties=my_font)\n",
    "plt.xlabel(\"Sentence length\", fontproperties=my_font)\n",
    "plt.ylabel(\"Frequency of sentence length\", fontproperties=my_font)\n",
    "plt.savefig(\"./Plot of sentence length and frequency.png\")\n",
    "plt.close()\n",
    "\n",
    "#  绘制句子长度累计分布函数（CDF）\n",
    "sent_pentage_list = [(count / sum(sent_freq)) for count in accumulate(sent_freq)]\n",
    "\n",
    "# 绘制CDF\n",
    "plt.plot(sent_length, sent_pentage_list)\n",
    "\n",
    "#  寻找分位点为quantile的句子长度\n",
    "quantile = 0.91\n",
    "# print(list(sent_pentage_list))\n",
    "for length, per in zip(sent_length, sent_pentage_list):\n",
    "    if round(per, 2) == quantile:\n",
    "        index = length\n",
    "        break\n",
    "print('\\nthe sentence length of %s quantile: %d' % (quantile, index))\n",
    "\n",
    "# 绘制句子长度累积分布函数图\n",
    "plt.plot(sent_length, sent_pentage_list)\n",
    "plt.hlines(quantile, 0, index, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.vlines(index, 0, quantile, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.text(0, quantile, str(quantile))\n",
    "plt.text(index, 0, str(index))\n",
    "plt.title(\"Plot of sentence length cumulative distribution function\", fontproperties=my_font)\n",
    "plt.xlabel(\"Sentence length\", fontproperties=my_font)\n",
    "plt.ylabel(\"Cumulative distribution of sentence length\", fontproperties=my_font)\n",
    "plt.savefig(\"./Plot of sentence length cumulative distribution function.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b665c",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa051031",
   "metadata": {},
   "source": [
    "## 2.1 Label sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b58f1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def emotion2sentiment(emotion):\n",
    "    if emotion == 'angry':\n",
    "        return 0\n",
    "    elif emotion == 'fear':\n",
    "        return 1\n",
    "    elif emotion == 'sad':\n",
    "        return 2\n",
    "    elif emotion == 'neural':\n",
    "        return 3\n",
    "    elif emotion == 'surprise':\n",
    "        return 4\n",
    "    elif emotion == 'happy':\n",
    "        return 5\n",
    "    \n",
    "def sentiment2emotion(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"angry\"\n",
    "    elif sentiment == 1:\n",
    "        return \"fear\"\n",
    "    elif sentiment == 2:\n",
    "        return \"sad\"\n",
    "    elif sentiment == 3:\n",
    "        return \"neural\"\n",
    "    elif sentiment == 4:\n",
    "        return \"surprise\"\n",
    "    elif sentiment == 5:\n",
    "        return \"happy\"\n",
    "    \n",
    "data_train['sentiment'] = data_train.emotion.apply(emotion2sentiment)\n",
    "print(type(data_train.emotion))\n",
    "# data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198df1b",
   "metadata": {},
   "source": [
    "## 2.2 Remove symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d543ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_symbol(s_line):\n",
    "#     # 剔除英文、数字，以及空格\n",
    "#     special_regex = re.compile(r\"[a-zA-Z0-9\\s]+\")\n",
    "#     # 剔除英文标点符号和特殊符号\n",
    "#     en_regex = re.compile(r\"[.…{|}#$%&\\'()*+,!-_./:~^;<=>?@★●，。]+\")\n",
    "#     # 剔除中文标点符号\n",
    "#     zn_regex = re.compile(r\"[《》、，“”；～？！：（）【】]+\")\n",
    "\n",
    "#     s_line = special_regex.sub(r\"\", s_line)\n",
    "#     s_line = en_regex.sub(r\"\", s_line)\n",
    "#     s_line = zn_regex.sub(r\"\", s_line)\n",
    "#     return s_line\n",
    "\n",
    "\n",
    "def remove_symbol(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    rule = re.compile(u\"[^a-zA-Z0-9\\u4E00-\\u9FA5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "\n",
    "\n",
    "data_train['remove_symbol_text'] = data_train.text.astype(str).apply(remove_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987aa5d0",
   "metadata": {},
   "source": [
    "## 2.3 Cut words by jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31afc8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Gao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.440 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "\n",
    "data_train['cut_text'] = data_train['remove_symbol_text'].astype(str).apply(chinese_word_cut)\n",
    "# data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dad89c",
   "metadata": {},
   "source": [
    "## 2.3 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3078b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data_train['cut_text']\n",
    "# y = data_train.sentiment\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=22)\n",
    "# X_train = data_train['cut_text']\n",
    "# y_train = data_train.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f92ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "stop_words_file = '哈工大停用词表.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "print(type(stopwords))\n",
    "vect = CountVectorizer(max_df = 0.8, \n",
    "                       min_df = 3, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=stopwords)\n",
    "# vect = TfidfVectorizer(use_idf=True, smooth_idf=True,max_df = 0.8, \n",
    "#                        min_df = 3, \n",
    "#                        token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "#                        stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e08741b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ac</th>\n",
       "      <th>ampquot</th>\n",
       "      <th>and</th>\n",
       "      <th>anti</th>\n",
       "      <th>app</th>\n",
       "      <th>apple</th>\n",
       "      <th>baby</th>\n",
       "      <th>bb</th>\n",
       "      <th>bgm</th>\n",
       "      <th>bitch</th>\n",
       "      <th>...</th>\n",
       "      <th>鼻孔</th>\n",
       "      <th>鼻屎</th>\n",
       "      <th>鼻梁</th>\n",
       "      <th>鼻涕</th>\n",
       "      <th>鼻炎</th>\n",
       "      <th>齐全</th>\n",
       "      <th>齐鲁晚报</th>\n",
       "      <th>齿轮</th>\n",
       "      <th>龌龊</th>\n",
       "      <th>龟速</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ac  ampquot  and  anti  app  apple  baby  bb  bgm  bitch  ...  鼻孔  鼻屎  鼻梁  \\\n",
       "0   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "1   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "2   0        0    0     0    1      0     0   0    0      0  ...   0   0   0   \n",
       "3   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "4   0        0    0     0    0      0     0   0    0      0  ...   0   0   0   \n",
       "\n",
       "   鼻涕  鼻炎  齐全  齐鲁晚报  齿轮  龌龊  龟速  \n",
       "0   0   0   0     0   0   0   0  \n",
       "1   0   0   0     0   0   0   0  \n",
       "2   0   0   0     0   0   0   0  \n",
       "3   0   0   0     0   0   0   0  \n",
       "4   0   0   0     0   0   0   0  \n",
       "\n",
       "[5 rows x 12371 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = pd.DataFrame(vect.fit_transform(X_train).toarray(), columns=vect.get_feature_names_out())\n",
    "# test.head()\n",
    "# vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf1433",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bca61",
   "metadata": {},
   "source": [
    "## 3.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b681ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7809939677680742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "X_train = data_train['cut_text']\n",
    "y_train = data_train.sentiment\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aaa27a",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab4da026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(22214, 12371)\n",
      "(22214,)\n",
      "(22214,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_vect))\n",
    "print(X_train_vect.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5102b",
   "metadata": {},
   "source": [
    "# 4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94838e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao\\anaconda3\\envs\\pytorch\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505221462009363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cut_text</th>\n",
       "      <th>nb_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>真正减肥就是这样，体重变化不重要，毕竟利尿剂也能做到，体型变化才是最牛的??，抽油的节奏</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>真正 减肥 就是 这样 ， 体重 变化 不 重要 ， 毕竟 利尿剂 也 能 做到 ， 体型 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>踏马的，之前床垫睡的不舒服，买了个新床垫，换上的时候发现，原来是之前的床垫垫反了……最近快半...</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>踏马 的 ， 之前 床垫 睡 的 不 舒服 ， 买 了 个 新 床垫 ， 换上 的 时候 发...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>高中时最好的朋友，又住院了又要做开颅手术，貌似情况更糟，她好害怕，她说：“还有很多想做的事情...</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>高中 时 最好 的 朋友 ， 又 住院 了 又 要 做 开颅 手术 ， 貌似 情况 更糟 ，...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>徐海乔曾经在访谈里描述过自己的这段经历，贵州深山的雨夜里拍摄一部这样题材的影片，想必感受一定...</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>徐海 乔 曾经 在 访谈 里 描述 过 自己 的 这段 经历 ， 贵州 深山 的 雨 夜里 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我好羡慕你能挥霍现实改变命运；你彷徨在我需仰望的天空中；你能飞得更高更远、这一切我并不是遥不...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>我 好羡慕 你 能 挥霍 现实 改变命运 ； 你 彷徨 在 我 需 仰望 的 天空 中 ； ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>女人玩相信自己力量无穷大饮水机上水没有什么难度。纪念一下。</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>女人 玩 相信 自己 力量 无穷大 饮水机 上 水 没有 什么 难度 。 纪念 一下 。</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>上一百斤的感受就是不穿鞋站在地板上脚丫都疼。</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>上 一百斤 的 感受 就是 不 穿鞋 站 在 地板 上 脚丫 都 疼 。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>在这个终级目标下，中医是用精气学说、阴阳学说和五行学说，这三大来自中国古典哲学的理论，来具体...</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "      <td>在 这个 终级 目标 下 ， 中医 是 用 精气 学说 、 阴阳 学说 和 五行 学说 ， ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>川南奉这种偏远路线出现使馆车，会不会是间谍？</td>\n",
       "      <td>surprise</td>\n",
       "      <td>4</td>\n",
       "      <td>川南 奉 这种 偏远 路线 出现 使馆 车 ， 会 不会 是 间谍 ？</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>花岗闪长岩的材料特征与花岗岩类似。</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "      <td>花岗 闪长岩 的 材料 特征 与 花岗岩 类似 。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>这手机有一点不好，不解开密码锁居然能拍照</td>\n",
       "      <td>surprise</td>\n",
       "      <td>4</td>\n",
       "      <td>这 手机 有 一点 不好 ， 不 解开 密码锁 居然 能 拍照</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>今天是回国后最放松的一天了，先是淘到很喜欢的粉红小鞋，后是整整睡了一下午的觉。然后朋友带去吃...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>今天 是 回国 后 最 放松 的 一天 了 ， 先是 淘到 很 喜欢 的 粉红 小鞋 ， 后...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>听到熟悉的旋律，想起高中入学之前的那个军训，好像那一年整个夏天都是火热的，我被晒得黝黑黝黑，...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>听到 熟悉 的 旋律 ， 想起 高中 入学 之前 的 那个 军训 ， 好像 那 一年 整个 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>快递被某个人自说自话拆开了，心里像吃了苍蝇屎一样窝瑟</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>快递 被 某个 人 自说自话 拆开 了 ， 心里 像 吃 了 苍蝇 屎 一样 窝瑟</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>做库存表，突然惊觉，竟然就快要进入12月了，思索了一下，为什么反射弧这么长？福！建！还！穿！...</td>\n",
       "      <td>angry</td>\n",
       "      <td>0</td>\n",
       "      <td>做 库存 表 ， 突然 惊觉 ， 竟然 就 快要 进入 12 月 了 ， 思索 了 一下 ，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>时间可以如初，有些事情，却难以如故</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>时间 可以 如初 ， 有些 事情 ， 却 难以 如故</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>危机有两种，当雷曼兄弟或两房崩溃时，人们不一定逃向金银，他们更可能投奔美元或美国国债，老夫称...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>危机 有 两种 ， 当 雷曼 兄弟 或 两房 崩溃 时 ， 人们 不 一定 逃向 金银 ， ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>第一次似乎也没那么吓人，总是躲避着新事物，其实经历过才发现原来还挺有趣的，性格好动的人怎么能...</td>\n",
       "      <td>happy</td>\n",
       "      <td>5</td>\n",
       "      <td>第一次 似乎 也 没 那么 吓人 ， 总是 躲避 着 新 事物 ， 其实 经历 过才 发现 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>不能看太过悲伤的电影，情绪都变得有点忧伤。里昂那么好，他不该死，他值得更好的生活</td>\n",
       "      <td>sad</td>\n",
       "      <td>2</td>\n",
       "      <td>不能 看太过 悲伤 的 电影 ， 情绪 都 变得 有点 忧伤 。 里昂 那么 好 ， 他 不...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>恐怖的存在，为什么没有人管，太吓人了！</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>恐怖 的 存在 ， 为什么 没有 人管 ， 太 吓人 了 ！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   emotion  sentiment  \\\n",
       "0        真正减肥就是这样，体重变化不重要，毕竟利尿剂也能做到，体型变化才是最牛的??，抽油的节奏     happy          5   \n",
       "1   踏马的，之前床垫睡的不舒服，买了个新床垫，换上的时候发现，原来是之前的床垫垫反了……最近快半...     angry          0   \n",
       "2   高中时最好的朋友，又住院了又要做开颅手术，貌似情况更糟，她好害怕，她说：“还有很多想做的事情...       sad          2   \n",
       "3   徐海乔曾经在访谈里描述过自己的这段经历，贵州深山的雨夜里拍摄一部这样题材的影片，想必感受一定...       sad          2   \n",
       "4   我好羡慕你能挥霍现实改变命运；你彷徨在我需仰望的天空中；你能飞得更高更远、这一切我并不是遥不...     happy          5   \n",
       "5                       女人玩相信自己力量无穷大饮水机上水没有什么难度。纪念一下。     happy          5   \n",
       "6                              上一百斤的感受就是不穿鞋站在地板上脚丫都疼。       sad          2   \n",
       "7   在这个终级目标下，中医是用精气学说、阴阳学说和五行学说，这三大来自中国古典哲学的理论，来具体...    neural          3   \n",
       "8                              川南奉这种偏远路线出现使馆车，会不会是间谍？  surprise          4   \n",
       "9                                   花岗闪长岩的材料特征与花岗岩类似。    neural          3   \n",
       "10                               这手机有一点不好，不解开密码锁居然能拍照  surprise          4   \n",
       "11  今天是回国后最放松的一天了，先是淘到很喜欢的粉红小鞋，后是整整睡了一下午的觉。然后朋友带去吃...     happy          5   \n",
       "12  听到熟悉的旋律，想起高中入学之前的那个军训，好像那一年整个夏天都是火热的，我被晒得黝黑黝黑，...     happy          5   \n",
       "13                         快递被某个人自说自话拆开了，心里像吃了苍蝇屎一样窝瑟     angry          0   \n",
       "14  做库存表，突然惊觉，竟然就快要进入12月了，思索了一下，为什么反射弧这么长？福！建！还！穿！...     angry          0   \n",
       "15                                  时间可以如初，有些事情，却难以如故       sad          2   \n",
       "16  危机有两种，当雷曼兄弟或两房崩溃时，人们不一定逃向金银，他们更可能投奔美元或美国国债，老夫称...      fear          1   \n",
       "17  第一次似乎也没那么吓人，总是躲避着新事物，其实经历过才发现原来还挺有趣的，性格好动的人怎么能...     happy          5   \n",
       "18           不能看太过悲伤的电影，情绪都变得有点忧伤。里昂那么好，他不该死，他值得更好的生活       sad          2   \n",
       "19                                恐怖的存在，为什么没有人管，太吓人了！      fear          1   \n",
       "\n",
       "                                             cut_text  nb_result  \n",
       "0   真正 减肥 就是 这样 ， 体重 变化 不 重要 ， 毕竟 利尿剂 也 能 做到 ， 体型 ...          5  \n",
       "1   踏马 的 ， 之前 床垫 睡 的 不 舒服 ， 买 了 个 新 床垫 ， 换上 的 时候 发...          4  \n",
       "2   高中 时 最好 的 朋友 ， 又 住院 了 又 要 做 开颅 手术 ， 貌似 情况 更糟 ，...          2  \n",
       "3   徐海 乔 曾经 在 访谈 里 描述 过 自己 的 这段 经历 ， 贵州 深山 的 雨 夜里 ...          1  \n",
       "4   我 好羡慕 你 能 挥霍 现实 改变命运 ； 你 彷徨 在 我 需 仰望 的 天空 中 ； ...          5  \n",
       "5        女人 玩 相信 自己 力量 无穷大 饮水机 上 水 没有 什么 难度 。 纪念 一下 。          5  \n",
       "6                上 一百斤 的 感受 就是 不 穿鞋 站 在 地板 上 脚丫 都 疼 。          0  \n",
       "7   在 这个 终级 目标 下 ， 中医 是 用 精气 学说 、 阴阳 学说 和 五行 学说 ， ...          3  \n",
       "8                 川南 奉 这种 偏远 路线 出现 使馆 车 ， 会 不会 是 间谍 ？          3  \n",
       "9                           花岗 闪长岩 的 材料 特征 与 花岗岩 类似 。          3  \n",
       "10                    这 手机 有 一点 不好 ， 不 解开 密码锁 居然 能 拍照          0  \n",
       "11  今天 是 回国 后 最 放松 的 一天 了 ， 先是 淘到 很 喜欢 的 粉红 小鞋 ， 后...          5  \n",
       "12  听到 熟悉 的 旋律 ， 想起 高中 入学 之前 的 那个 军训 ， 好像 那 一年 整个 ...          2  \n",
       "13          快递 被 某个 人 自说自话 拆开 了 ， 心里 像 吃 了 苍蝇 屎 一样 窝瑟          0  \n",
       "14  做 库存 表 ， 突然 惊觉 ， 竟然 就 快要 进入 12 月 了 ， 思索 了 一下 ，...          0  \n",
       "15                         时间 可以 如初 ， 有些 事情 ， 却 难以 如故          2  \n",
       "16  危机 有 两种 ， 当 雷曼 兄弟 或 两房 崩溃 时 ， 人们 不 一定 逃向 金银 ， ...          3  \n",
       "17  第一次 似乎 也 没 那么 吓人 ， 总是 躲避 着 新 事物 ， 其实 经历 过才 发现 ...          1  \n",
       "18  不能 看太过 悲伤 的 电影 ， 情绪 都 变得 有点 忧伤 。 里昂 那么 好 ， 他 不...          2  \n",
       "19                     恐怖 的 存在 ， 为什么 没有 人管 ， 太 吓人 了 ！          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_excel('D:\\\\files\\\\Courses\\\\Final Year Project\\\\archive\\\\test.xlsx')\n",
    "data_test['sentiment'] = data_test.emotion.apply(emotion2sentiment)\n",
    "data_test['cut_text'] = data_test.text.astype(str).apply(chinese_word_cut)\n",
    "X_test = data_test['cut_text']\n",
    "y_test = data_test.sentiment\n",
    "X_test_vect = vect.transform(X_test)\n",
    "print(nb.score(X_test_vect, y_test))\n",
    "nb_result = nb.predict(X_test_vect)\n",
    "data_test['nb_result'] = nb_result\n",
    "data_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96712096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "m_tp: 1334.0\n",
      "m_fp: 635.0\n",
      "m_fn: 358.0\n",
      "m_tn: 3227.0\n",
      "fear\n",
      "m_tp: 123.0\n",
      "m_fp: 68.0\n",
      "m_fn: 119.0\n",
      "m_tn: 5244.0\n",
      "sad\n",
      "m_tp: 544.0\n",
      "m_fp: 519.0\n",
      "m_fn: 420.0\n",
      "m_tn: 4071.0\n",
      "neural\n",
      "m_tp: 785.0\n",
      "m_fp: 155.0\n",
      "m_fn: 362.0\n",
      "m_tn: 4252.0\n",
      "surprise\n",
      "m_tp: 141.0\n",
      "m_fp: 118.0\n",
      "m_fn: 299.0\n",
      "m_tn: 4996.0\n",
      "happy\n",
      "m_tp: 686.0\n",
      "m_fp: 446.0\n",
      "m_fn: 383.0\n",
      "m_tn: 4039.0\n",
      "overall accuracy: 0.6505221462009363\n",
      "overall precision: 0.6364590823240035\n",
      "overall recall: 0.5845942904835666\n",
      "overall f_measure: 0.6094251962099261\n",
      "angry\n",
      "0.8212099387828592\n",
      "0.6775012696800407\n",
      "0.7884160756501182\n",
      "0.7287626331603386\n",
      "fear\n",
      "0.9663305725603168\n",
      "0.643979057591623\n",
      "0.5082644628099173\n",
      "0.5681293302540416\n",
      "sad\n",
      "0.8309326611451207\n",
      "0.5117591721542804\n",
      "0.5643153526970954\n",
      "0.5367538233843118\n",
      "neural\n",
      "0.9069139359020526\n",
      "0.8351063829787234\n",
      "0.6843940714908456\n",
      "0.7522759942501198\n",
      "surprise\n",
      "0.9249189773136478\n",
      "0.5444015444015444\n",
      "0.32045454545454544\n",
      "0.4034334763948498\n",
      "happy\n",
      "0.8507382066978754\n",
      "0.6060070671378092\n",
      "0.6417212347988774\n",
      "0.62335302135393\n",
      "accuracy 0.6505221462009363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.68      0.79      0.73      1692\n",
      "        fear       0.64      0.51      0.57       242\n",
      "         sad       0.51      0.56      0.54       964\n",
      "      neural       0.84      0.68      0.75      1147\n",
      "    surprise       0.54      0.32      0.40       440\n",
      "       happy       0.61      0.64      0.62      1069\n",
      "\n",
      "    accuracy                           0.65      5554\n",
      "   macro avg       0.64      0.58      0.60      5554\n",
      "weighted avg       0.66      0.65      0.65      5554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_matrix(ls):\n",
    "    eval_matrix = np.zeros((6, 6))\n",
    "    for m_i in range(len(ls)):\n",
    "        m_row = ls['sentiment'][m_i]\n",
    "        m_col = ls['nb_result'][m_i]\n",
    "        eval_matrix[m_row][m_col] += 1\n",
    "    return eval_matrix\n",
    "#     print(eval_matrix_nb) \n",
    "\n",
    "def get_evaluation(matrix):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    tn = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f_measure = []\n",
    "    for m_i in range(6):\n",
    "        m_tp = matrix[m_i][m_i]\n",
    "        m_fp = np.sum(matrix, axis=0)[m_i] - matrix[m_i][m_i]\n",
    "        m_fn = np.sum(matrix, axis=1)[m_i] - matrix[m_i][m_i]\n",
    "        m_tn = np.sum(matrix) - m_tp - m_fp - m_fn\n",
    "        m_accuracy = (m_tp + m_tn) / (m_tp + m_fp + m_fn + m_tn)\n",
    "        m_precision = (m_tp) / (m_tp + m_fp)\n",
    "        m_recall = m_tp / (m_tp + m_fn)\n",
    "        m_f_measure = 2 * m_precision * m_recall / (m_precision + m_recall)\n",
    "        accuracy.append(m_accuracy)\n",
    "        precision.append(m_precision)\n",
    "        recall.append(m_recall)\n",
    "        f_measure.append(m_f_measure)\n",
    "        print(sentiment2emotion(m_i))\n",
    "        print(\"m_tp: \" + str(m_tp))\n",
    "        print(\"m_fp: \" + str(m_fp))\n",
    "        print(\"m_fn: \" + str(m_fn))\n",
    "        print(\"m_tn: \" + str(m_tn))\n",
    "    overall_accuracy = np.trace(matrix) / np.sum(matrix)\n",
    "    overall_precision = np.sum(precision) / len(precision)\n",
    "    overall_recall = np.sum(recall) / len(recall)\n",
    "    overall_f_measure = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "    return overall_accuracy, overall_precision, overall_recall, overall_f_measure, accuracy, precision, recall, f_measure\n",
    "\n",
    "eval_matrix_nb = get_evaluation_matrix(data_test)\n",
    "overall_accuracy_nb, overall_precision_nb, overall_recall_nb, overall_f_measure_nb, accuracy_nb, precision_nb, recall_nb, f_measure_nb = get_evaluation(eval_matrix_nb)\n",
    "print(\"overall accuracy: \" + str(overall_accuracy_nb))\n",
    "print(\"overall precision: \" + str(overall_precision_nb))\n",
    "print(\"overall recall: \" + str(overall_recall_nb))\n",
    "print(\"overall f_measure: \" + str(overall_f_measure_nb))\n",
    "for i in range(6):\n",
    "    emotion_type = sentiment2emotion(i)\n",
    "    print(emotion_type)\n",
    "    print(str(accuracy_nb[i]))\n",
    "    print(str(precision_nb[i]))\n",
    "    print(str(recall_nb[i]))\n",
    "    print(str(f_measure_nb[i]))\n",
    "#     print(\"accuracy: \" + str(accuracy_nb[i]))\n",
    "#     print(\"precision: \" + str(precision_nb[i]))\n",
    "#     print(\"recall: \" + str(recall_nb[i]))\n",
    "#     print(\"f_measure: \" + str(f_measure_nb[i]))\n",
    "\n",
    "\n",
    "from  sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "print('accuracy %s' % accuracy_score(data_test['nb_result'], data_test['sentiment']))\n",
    "# print(type(range(6)))\n",
    "tg_names = pd.Series(range(len(data_test.emotion.unique()))).apply(sentiment2emotion)\n",
    "print(classification_report(data_test['sentiment'], data_test['nb_result'], target_names=tg_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63fc25",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470a6cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the sentence:这场电影好好看\n",
      "The emotion of the sentence is happy\n"
     ]
    }
   ],
   "source": [
    "test_input = input(\"input the sentence:\")\n",
    "# test_input = \"电影真好看\"\n",
    "test_cut = chinese_word_cut(remove_symbol(test_input))\n",
    "# print(type(test_cut))\n",
    "test_vec = vect.transform([test_cut])\n",
    "# print(type(test_vec))\n",
    "res = nb.predict(test_vec)\n",
    "# print(res)\n",
    "print(\"The emotion of the sentence is \" + sentiment2emotion(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c65ad",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6981cd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6652, 3: 4602, 5: 4310, 2: 4026, 4: 1646, 1: 978})\n",
      "Counter({0: 1692, 3: 1147, 5: 1069, 2: 964, 4: 440, 1: 242})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "arr = Counter(y_train)\n",
    "print(arr)\n",
    "arr = Counter(y_test)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de0b166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5554.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(eval_matrix_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05688cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy', 'angry', 'sad', ..., 'neural', 'happy', 'happy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f393bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
