{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4db86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813e970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion2sentiment(emotion):\n",
    "    if emotion == 'angry':\n",
    "        return 0\n",
    "    elif emotion == 'fear':\n",
    "        return 1\n",
    "    elif emotion == 'sad':\n",
    "        return 2\n",
    "    elif emotion == 'neural':\n",
    "        return 3\n",
    "    elif emotion == 'surprise':\n",
    "        return 4\n",
    "    elif emotion == 'happy':\n",
    "        return 5\n",
    "    \n",
    "def sentiment2emotion(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"angry\"\n",
    "    elif sentiment == 1:\n",
    "        return \"fear\"\n",
    "    elif sentiment == 2:\n",
    "        return \"sad\"\n",
    "    elif sentiment == 3:\n",
    "        return \"neural\"\n",
    "    elif sentiment == 4:\n",
    "        return \"surprise\"\n",
    "    elif sentiment == 5:\n",
    "        return \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ab362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbol(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    rule = re.compile(u\"[^a-zA-Z0-9\\u4E00-\\u9FA5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cb9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70119893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "stop_words_file = '哈工大停用词表.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "# # print(type(stopwords))\n",
    "# vect = CountVectorizer(max_df = 0.8, \n",
    "#                        min_df = 3, \n",
    "#                        token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "#                        stop_words=stopwords)\n",
    "# vect = TfidfVectorizer(use_idf=True, smooth_idf=True,max_df = 0.8, \n",
    "#                        min_df = 3, \n",
    "#                        token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "#                        stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728c3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置最频繁使用的50000个词\n",
    "MAX_NB_WORDS = 50000\n",
    "# 每条cut_review最大的长度\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# 设置Embeddingceng层的维度\n",
    "EMBEDDING_DIM = 100\n",
    " \n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "# tokenizer.fit_on_texts(data_train['cut_text'].values)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('共有 %s 个不相同的词语.' % len(word_index))\n",
    "def load_tokenizer():\n",
    "    f = open('tokenizer_json.json', 'r')\n",
    "    content = f.read()\n",
    "#     print(type(content))\n",
    "    f.close()\n",
    "    _tokenizer = keras.preprocessing.text.tokenizer_from_json(content)\n",
    "    return _tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb3b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def load_model():\n",
    "    model = keras.models.load_model(\"LSTM_model_20230301\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b12ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(test_input):\n",
    "    # test_input = input(\"input the sentence:\")\n",
    "#     test_input = \"今天真开心\"\n",
    "    test_cut = chinese_word_cut(remove_symbol(test_input))\n",
    "    # print(type(test_cut))\n",
    "    test_array = np.array([test_cut])\n",
    "    test_X = my_tokenizer.texts_to_sequences(test_array)\n",
    "    # print(len(test_X))\n",
    "    test_X = pad_sequences(test_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    # print(test_X.shape)\n",
    "    result_array = my_model.predict(test_X).argmax(axis=1)\n",
    "    # print(result_array)\n",
    "    result_emotion = sentiment2emotion(result_array[0])\n",
    "    return result_emotion\n",
    "    # print(result_emotion)\n",
    "    # result_sentiment = np.argmax(np.bincount(result_array))\n",
    "    # result_emotion = sentiment2emotion(result_sentiment)\n",
    "    # # result = reconstructed_model.predict(test_X)\n",
    "    # # ticklabels = pd.Series(range(len(data_test.emotion.unique()))).apply(sentiment2emotion)\n",
    "    # print(result_emotion)\n",
    "    # # print(type(result))\n",
    "    # # print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93dc6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    global my_tokenizer\n",
    "    global my_model\n",
    "    my_tokenizer = load_tokenizer()\n",
    "    my_model = load_model()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192c661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init()\n",
    "# # text = input(\"Please input the text: \")\n",
    "# res = my_predict(\"生气\")\n",
    "# print(\"The emotion of the text is \" + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7e0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PySimpleGUI as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47aefbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Gao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.480 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "sg.theme('GrayGrayGray')   # 设置当前主题\n",
    "# 界面布局，将会按照列表顺序从上往下依次排列，二级列表中，从左往右依此排列\n",
    "layout = [  [sg.Text('Please input the text: '), sg.InputText(key='-IN-')],\n",
    "            [sg.Text(key='-OUTPUT-')],\n",
    "            [sg.Button('Ok'), sg.Button('Cancel')] ]\n",
    "\n",
    "# 创造窗口\n",
    "window = sg.Window('Automatic Sentiment Analysis of Text', layout)\n",
    "# 事件循环并获取输入值\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event in (None, 'Cancel'):   # 如果用户关闭窗口或点击`Cancel`\n",
    "        break\n",
    "    if event == 'Ok':\n",
    "        res = my_predict(values['-IN-'])\n",
    "        window['-OUTPUT-'].update(\"The sentiment of the input text is \" + res)\n",
    "    \n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6731278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg.theme_previewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f052c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
